{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "aIQSz4uOAsik"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvsOP7TS5bEz"
      },
      "source": [
        "[Problem 1] Looking back on the scratch, Preparing dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWN1b4k-6P94",
        "outputId": "ec6132f7-54c8-434b-a615-b23da760370a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"Iris.csv\")\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)\n",
        "print(y_train_one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AlrX-cw7JMR"
      },
      "source": [
        "[Problem 2] Consider the correspondence between Scratch and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9KoiO4j6wew",
        "outputId": "15010d51-aaee-48e0-cab9-5deda04fb84f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-c643c55deb19>:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        }
      ],
      "source": [
        "class SampleIterator():\n",
        "    def __init__(self):\n",
        "        self.X = [1,2,3,4,5]\n",
        "        self.counter = 0\n",
        "        self.stop = len(self.X)\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self.counter >= self.stop:\n",
        "            raise StopIteration()\n",
        "        x = self.X[self.counter]\n",
        "        self.counter += 1\n",
        "        return x\n",
        "sample_iter = SampleIterator()\n",
        "for x in sample_iter:\n",
        "    print(x)\n",
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "#Hyperparameters \n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 2\n",
        "\n",
        "#determine the shape of the network inputs and outputs\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "#Mini-batch generation iterator\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQzIqJVT-uIy",
        "outputId": "0768bc8e-f6c8-4902-b4bc-68cb4b46e16f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 3.5637, val_loss : 26.8285, acc : 0.500\n",
            "Epoch 1, loss : 3.0520, val_loss : 22.5670, acc : 0.500\n",
            "Epoch 2, loss : 2.5474, val_loss : 18.3268, acc : 0.500\n",
            "Epoch 3, loss : 2.0544, val_loss : 14.2706, acc : 0.531\n",
            "Epoch 4, loss : 1.5753, val_loss : 10.2950, acc : 0.531\n",
            "Epoch 5, loss : 1.1117, val_loss : 6.4828, acc : 0.562\n",
            "Epoch 6, loss : 0.7091, val_loss : 3.9602, acc : 0.688\n",
            "Epoch 7, loss : 0.4155, val_loss : 2.0388, acc : 0.750\n",
            "Epoch 8, loss : 0.2298, val_loss : 0.6814, acc : 0.875\n",
            "Epoch 9, loss : 0.1142, val_loss : 0.1106, acc : 0.938\n",
            "Epoch 10, loss : 0.0697, val_loss : 0.0146, acc : 1.000\n",
            "Epoch 11, loss : 0.0647, val_loss : 0.0443, acc : 0.969\n",
            "Epoch 12, loss : 0.0612, val_loss : 0.0381, acc : 0.969\n",
            "Epoch 13, loss : 0.0558, val_loss : 0.0230, acc : 1.000\n",
            "Epoch 14, loss : 0.0510, val_loss : 0.0169, acc : 1.000\n",
            "Epoch 15, loss : 0.0470, val_loss : 0.0163, acc : 1.000\n",
            "Epoch 16, loss : 0.0432, val_loss : 0.0158, acc : 1.000\n",
            "Epoch 17, loss : 0.0395, val_loss : 0.0156, acc : 1.000\n",
            "Epoch 18, loss : 0.0359, val_loss : 0.0154, acc : 1.000\n",
            "Epoch 19, loss : 0.0327, val_loss : 0.0152, acc : 1.000\n",
            "Epoch 20, loss : 0.0296, val_loss : 0.0150, acc : 1.000\n",
            "Epoch 21, loss : 0.0267, val_loss : 0.0145, acc : 1.000\n",
            "Epoch 22, loss : 0.0240, val_loss : 0.0136, acc : 1.000\n",
            "Epoch 23, loss : 0.0217, val_loss : 0.0123, acc : 1.000\n",
            "Epoch 24, loss : 0.0197, val_loss : 0.0110, acc : 1.000\n",
            "Epoch 25, loss : 0.0179, val_loss : 0.0098, acc : 1.000\n",
            "Epoch 26, loss : 0.0164, val_loss : 0.0088, acc : 1.000\n",
            "Epoch 27, loss : 0.0150, val_loss : 0.0080, acc : 1.000\n",
            "Epoch 28, loss : 0.0138, val_loss : 0.0075, acc : 1.000\n",
            "Epoch 29, loss : 0.0128, val_loss : 0.0072, acc : 1.000\n",
            "Epoch 30, loss : 0.0119, val_loss : 0.0071, acc : 1.000\n",
            "Epoch 31, loss : 0.0110, val_loss : 0.0070, acc : 1.000\n",
            "Epoch 32, loss : 0.0103, val_loss : 0.0071, acc : 1.000\n",
            "Epoch 33, loss : 0.0097, val_loss : 0.0073, acc : 1.000\n",
            "Epoch 34, loss : 0.0091, val_loss : 0.0075, acc : 1.000\n",
            "Epoch 35, loss : 0.0085, val_loss : 0.0078, acc : 1.000\n",
            "Epoch 36, loss : 0.0081, val_loss : 0.0081, acc : 1.000\n",
            "Epoch 37, loss : 0.0077, val_loss : 0.0085, acc : 1.000\n",
            "Epoch 38, loss : 0.0073, val_loss : 0.0089, acc : 1.000\n",
            "Epoch 39, loss : 0.0069, val_loss : 0.0094, acc : 1.000\n",
            "Epoch 40, loss : 0.0067, val_loss : 0.0098, acc : 1.000\n",
            "Epoch 41, loss : 0.0064, val_loss : 0.0103, acc : 1.000\n",
            "Epoch 42, loss : 0.0061, val_loss : 0.0108, acc : 1.000\n",
            "Epoch 43, loss : 0.0059, val_loss : 0.0113, acc : 1.000\n",
            "Epoch 44, loss : 0.0057, val_loss : 0.0117, acc : 1.000\n",
            "Epoch 45, loss : 0.0055, val_loss : 0.0121, acc : 1.000\n",
            "Epoch 46, loss : 0.0054, val_loss : 0.0126, acc : 1.000\n",
            "Epoch 47, loss : 0.0052, val_loss : 0.0130, acc : 1.000\n",
            "Epoch 48, loss : 0.0051, val_loss : 0.0134, acc : 1.000\n",
            "Epoch 49, loss : 0.0049, val_loss : 0.0136, acc : 1.000\n",
            "Epoch 50, loss : 0.0048, val_loss : 0.0138, acc : 1.000\n",
            "Epoch 51, loss : 0.0047, val_loss : 0.0139, acc : 1.000\n",
            "Epoch 52, loss : 0.0046, val_loss : 0.0140, acc : 1.000\n",
            "Epoch 53, loss : 0.0045, val_loss : 0.0141, acc : 1.000\n",
            "Epoch 54, loss : 0.0044, val_loss : 0.0142, acc : 1.000\n",
            "Epoch 55, loss : 0.0043, val_loss : 0.0142, acc : 1.000\n",
            "Epoch 56, loss : 0.0043, val_loss : 0.0142, acc : 1.000\n",
            "Epoch 57, loss : 0.0042, val_loss : 0.0141, acc : 1.000\n",
            "Epoch 58, loss : 0.0041, val_loss : 0.0141, acc : 1.000\n",
            "Epoch 59, loss : 0.0040, val_loss : 0.0141, acc : 1.000\n",
            "Epoch 60, loss : 0.0040, val_loss : 0.0140, acc : 1.000\n",
            "Epoch 61, loss : 0.0039, val_loss : 0.0140, acc : 1.000\n",
            "Epoch 62, loss : 0.0039, val_loss : 0.0140, acc : 1.000\n",
            "Epoch 63, loss : 0.0038, val_loss : 0.0140, acc : 1.000\n",
            "Epoch 64, loss : 0.0037, val_loss : 0.0139, acc : 1.000\n",
            "Epoch 65, loss : 0.0037, val_loss : 0.0138, acc : 1.000\n",
            "Epoch 66, loss : 0.0036, val_loss : 0.0138, acc : 1.000\n",
            "Epoch 67, loss : 0.0036, val_loss : 0.0137, acc : 1.000\n",
            "Epoch 68, loss : 0.0035, val_loss : 0.0137, acc : 1.000\n",
            "Epoch 69, loss : 0.0035, val_loss : 0.0136, acc : 1.000\n",
            "Epoch 70, loss : 0.0034, val_loss : 0.0135, acc : 1.000\n",
            "Epoch 71, loss : 0.0034, val_loss : 0.0135, acc : 1.000\n",
            "Epoch 72, loss : 0.0033, val_loss : 0.0134, acc : 1.000\n",
            "Epoch 73, loss : 0.0033, val_loss : 0.0134, acc : 1.000\n",
            "Epoch 74, loss : 0.0032, val_loss : 0.0133, acc : 1.000\n",
            "Epoch 75, loss : 0.0032, val_loss : 0.0133, acc : 1.000\n",
            "Epoch 76, loss : 0.0031, val_loss : 0.0132, acc : 1.000\n",
            "Epoch 77, loss : 0.0031, val_loss : 0.0132, acc : 1.000\n",
            "Epoch 78, loss : 0.0031, val_loss : 0.0131, acc : 1.000\n",
            "Epoch 79, loss : 0.0030, val_loss : 0.0130, acc : 1.000\n",
            "Epoch 80, loss : 0.0030, val_loss : 0.0130, acc : 1.000\n",
            "Epoch 81, loss : 0.0030, val_loss : 0.0130, acc : 1.000\n",
            "Epoch 82, loss : 0.0029, val_loss : 0.0129, acc : 1.000\n",
            "Epoch 83, loss : 0.0029, val_loss : 0.0128, acc : 1.000\n",
            "Epoch 84, loss : 0.0028, val_loss : 0.0128, acc : 1.000\n",
            "Epoch 85, loss : 0.0028, val_loss : 0.0128, acc : 1.000\n",
            "Epoch 86, loss : 0.0028, val_loss : 0.0127, acc : 1.000\n",
            "Epoch 87, loss : 0.0027, val_loss : 0.0127, acc : 1.000\n",
            "Epoch 88, loss : 0.0027, val_loss : 0.0126, acc : 1.000\n",
            "Epoch 89, loss : 0.0027, val_loss : 0.0126, acc : 1.000\n",
            "Epoch 90, loss : 0.0027, val_loss : 0.0125, acc : 1.000\n",
            "Epoch 91, loss : 0.0026, val_loss : 0.0125, acc : 1.000\n",
            "Epoch 92, loss : 0.0026, val_loss : 0.0125, acc : 1.000\n",
            "Epoch 93, loss : 0.0026, val_loss : 0.0124, acc : 1.000\n",
            "Epoch 94, loss : 0.0025, val_loss : 0.0124, acc : 1.000\n",
            "Epoch 95, loss : 0.0025, val_loss : 0.0124, acc : 1.000\n",
            "Epoch 96, loss : 0.0025, val_loss : 0.0123, acc : 1.000\n",
            "Epoch 97, loss : 0.0025, val_loss : 0.0123, acc : 1.000\n",
            "Epoch 98, loss : 0.0024, val_loss : 0.0122, acc : 1.000\n",
            "Epoch 99, loss : 0.0024, val_loss : 0.0122, acc : 1.000\n",
            "test_acc : 0.925\n"
          ]
        }
      ],
      "source": [
        "def example_net(x):  \n",
        "    tf.random.set_random_seed(0)  \n",
        "    weights = {  \n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),  \n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),  \n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))  \n",
        "    }  \n",
        "    biases = {  \n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),  \n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),  \n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))  \n",
        "    }  \n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])  \n",
        "    layer_1 = tf.nn.relu(layer_1)  \n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])  \n",
        "    layer_2 = tf.nn.relu(layer_2)  \n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "logits = example_net(X)  \n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))  \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  \n",
        "train_op = optimizer.minimize(loss_op)  \n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))   \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))  \n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:  \n",
        "    sess.run(init)  \n",
        "    for epoch in range(num_epochs):  \n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)  \n",
        "        total_loss = 0  \n",
        "        total_acc = 0  \n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):  \n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})  \n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})  \n",
        "            total_loss += loss  \n",
        "        total_loss /= n_samples  \n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})  \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))  \n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})  \n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
        "\n",
        "with tf.Session()  as sess: \n",
        "    sess.run(init)  \n",
        "    for epoch in  range(num_epochs):\n",
        "        for i,  (mini_batch_x, mini_batch_y)  in enumerate(get_mini_batch_train): \n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Z1hNmN_Osy"
      },
      "source": [
        "[Problem 3] Create a model of Iris using all three types of objective variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_8a-T1R_Vnd",
        "outputId": "ec9df85f-bff9-45df-9697-e82ae89d8d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1]\n",
            "[[1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\") | (df[\"Species\"] == \"Iris-setosa\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y[y == \"Iris-setosa\"] = 2\n",
        "print(y)\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)\n",
        "print(y_train_one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XskpKCkApQk",
        "outputId": "8f69d86c-e8f3-453c-e044-b56537ebad4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-39-100e96cdef15>:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.Tensor'>\n",
            "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
            "Epoch 0, loss : 1.9347, val_loss : 17.0133, acc : 0.583\n",
            "Epoch 1, loss : 1.4827, val_loss : 12.8771, acc : 0.597\n",
            "Epoch 2, loss : 1.0699, val_loss : 9.2645, acc : 0.681\n",
            "Epoch 3, loss : 0.7592, val_loss : 6.6297, acc : 0.778\n",
            "Epoch 4, loss : 0.5084, val_loss : 4.3544, acc : 0.736\n",
            "Epoch 5, loss : 0.3318, val_loss : 2.8181, acc : 0.722\n",
            "Epoch 6, loss : 0.2172, val_loss : 1.5843, acc : 0.736\n",
            "Epoch 7, loss : 0.1013, val_loss : 0.6282, acc : 0.847\n",
            "Epoch 8, loss : 0.0467, val_loss : 0.3417, acc : 0.861\n",
            "Epoch 9, loss : 0.0346, val_loss : 0.2806, acc : 0.875\n",
            "Epoch 10, loss : 0.0292, val_loss : 0.2521, acc : 0.903\n",
            "Epoch 11, loss : 0.0252, val_loss : 0.2174, acc : 0.917\n",
            "Epoch 12, loss : 0.0224, val_loss : 0.1915, acc : 0.931\n",
            "Epoch 13, loss : 0.0202, val_loss : 0.1785, acc : 0.931\n",
            "Epoch 14, loss : 0.0184, val_loss : 0.1648, acc : 0.931\n",
            "Epoch 15, loss : 0.0168, val_loss : 0.1539, acc : 0.931\n",
            "Epoch 16, loss : 0.0154, val_loss : 0.1456, acc : 0.931\n",
            "Epoch 17, loss : 0.0141, val_loss : 0.1373, acc : 0.944\n",
            "Epoch 18, loss : 0.0130, val_loss : 0.1298, acc : 0.958\n",
            "Epoch 19, loss : 0.0121, val_loss : 0.1233, acc : 0.958\n",
            "Epoch 20, loss : 0.0113, val_loss : 0.1164, acc : 0.958\n",
            "Epoch 21, loss : 0.0106, val_loss : 0.1097, acc : 0.958\n",
            "Epoch 22, loss : 0.0100, val_loss : 0.1043, acc : 0.958\n",
            "Epoch 23, loss : 0.0095, val_loss : 0.1002, acc : 0.958\n",
            "Epoch 24, loss : 0.0090, val_loss : 0.0974, acc : 0.958\n",
            "Epoch 25, loss : 0.0086, val_loss : 0.0952, acc : 0.958\n",
            "Epoch 26, loss : 0.0082, val_loss : 0.0934, acc : 0.958\n",
            "Epoch 27, loss : 0.0079, val_loss : 0.0924, acc : 0.958\n",
            "Epoch 28, loss : 0.0076, val_loss : 0.0914, acc : 0.958\n",
            "Epoch 29, loss : 0.0073, val_loss : 0.0909, acc : 0.958\n",
            "Epoch 30, loss : 0.0071, val_loss : 0.0904, acc : 0.958\n",
            "Epoch 31, loss : 0.0068, val_loss : 0.0901, acc : 0.958\n",
            "Epoch 32, loss : 0.0066, val_loss : 0.0898, acc : 0.958\n",
            "Epoch 33, loss : 0.0064, val_loss : 0.0898, acc : 0.958\n",
            "Epoch 34, loss : 0.0062, val_loss : 0.0898, acc : 0.958\n",
            "Epoch 35, loss : 0.0060, val_loss : 0.0900, acc : 0.958\n",
            "Epoch 36, loss : 0.0059, val_loss : 0.0900, acc : 0.958\n",
            "Epoch 37, loss : 0.0057, val_loss : 0.0900, acc : 0.958\n",
            "Epoch 38, loss : 0.0055, val_loss : 0.0906, acc : 0.958\n",
            "Epoch 39, loss : 0.0054, val_loss : 0.0914, acc : 0.944\n",
            "Epoch 40, loss : 0.0053, val_loss : 0.0918, acc : 0.958\n",
            "Epoch 41, loss : 0.0051, val_loss : 0.0922, acc : 0.958\n",
            "Epoch 42, loss : 0.0050, val_loss : 0.0931, acc : 0.958\n",
            "Epoch 43, loss : 0.0049, val_loss : 0.0941, acc : 0.958\n",
            "Epoch 44, loss : 0.0049, val_loss : 0.0946, acc : 0.958\n",
            "Epoch 45, loss : 0.0048, val_loss : 0.0953, acc : 0.958\n",
            "Epoch 46, loss : 0.0047, val_loss : 0.0965, acc : 0.958\n",
            "Epoch 47, loss : 0.0046, val_loss : 0.0971, acc : 0.958\n",
            "Epoch 48, loss : 0.0046, val_loss : 0.0979, acc : 0.958\n",
            "Epoch 49, loss : 0.0045, val_loss : 0.0986, acc : 0.958\n",
            "Epoch 50, loss : 0.0044, val_loss : 0.0993, acc : 0.958\n",
            "Epoch 51, loss : 0.0044, val_loss : 0.1003, acc : 0.958\n",
            "Epoch 52, loss : 0.0043, val_loss : 0.1008, acc : 0.958\n",
            "Epoch 53, loss : 0.0043, val_loss : 0.1015, acc : 0.958\n",
            "Epoch 54, loss : 0.0042, val_loss : 0.1024, acc : 0.958\n",
            "Epoch 55, loss : 0.0041, val_loss : 0.1030, acc : 0.958\n",
            "Epoch 56, loss : 0.0041, val_loss : 0.1041, acc : 0.958\n",
            "Epoch 57, loss : 0.0041, val_loss : 0.1046, acc : 0.958\n",
            "Epoch 58, loss : 0.0040, val_loss : 0.1049, acc : 0.958\n",
            "Epoch 59, loss : 0.0040, val_loss : 0.1059, acc : 0.958\n",
            "Epoch 60, loss : 0.0039, val_loss : 0.1065, acc : 0.958\n",
            "Epoch 61, loss : 0.0039, val_loss : 0.1070, acc : 0.958\n",
            "Epoch 62, loss : 0.0039, val_loss : 0.1077, acc : 0.958\n",
            "Epoch 63, loss : 0.0038, val_loss : 0.1084, acc : 0.958\n",
            "Epoch 64, loss : 0.0038, val_loss : 0.1086, acc : 0.958\n",
            "Epoch 65, loss : 0.0037, val_loss : 0.1089, acc : 0.958\n",
            "Epoch 66, loss : 0.0037, val_loss : 0.1094, acc : 0.958\n",
            "Epoch 67, loss : 0.0037, val_loss : 0.1102, acc : 0.958\n",
            "Epoch 68, loss : 0.0036, val_loss : 0.1105, acc : 0.958\n",
            "Epoch 69, loss : 0.0036, val_loss : 0.1109, acc : 0.958\n",
            "Epoch 70, loss : 0.0036, val_loss : 0.1114, acc : 0.958\n",
            "Epoch 71, loss : 0.0035, val_loss : 0.1118, acc : 0.958\n",
            "Epoch 72, loss : 0.0035, val_loss : 0.1123, acc : 0.958\n",
            "Epoch 73, loss : 0.0035, val_loss : 0.1128, acc : 0.958\n",
            "Epoch 74, loss : 0.0035, val_loss : 0.1132, acc : 0.958\n",
            "Epoch 75, loss : 0.0034, val_loss : 0.1136, acc : 0.958\n",
            "Epoch 76, loss : 0.0034, val_loss : 0.1138, acc : 0.958\n",
            "Epoch 77, loss : 0.0034, val_loss : 0.1141, acc : 0.958\n",
            "Epoch 78, loss : 0.0033, val_loss : 0.1145, acc : 0.958\n",
            "Epoch 79, loss : 0.0033, val_loss : 0.1147, acc : 0.958\n",
            "Epoch 80, loss : 0.0033, val_loss : 0.1153, acc : 0.958\n",
            "Epoch 81, loss : 0.0033, val_loss : 0.1150, acc : 0.958\n",
            "Epoch 82, loss : 0.0032, val_loss : 0.1157, acc : 0.958\n",
            "Epoch 83, loss : 0.0032, val_loss : 0.1165, acc : 0.958\n",
            "Epoch 84, loss : 0.0032, val_loss : 0.1162, acc : 0.958\n",
            "Epoch 85, loss : 0.0032, val_loss : 0.1169, acc : 0.958\n",
            "Epoch 86, loss : 0.0031, val_loss : 0.1171, acc : 0.958\n",
            "Epoch 87, loss : 0.0031, val_loss : 0.1172, acc : 0.958\n",
            "Epoch 88, loss : 0.0031, val_loss : 0.1180, acc : 0.958\n",
            "Epoch 89, loss : 0.0031, val_loss : 0.1184, acc : 0.958\n",
            "Epoch 90, loss : 0.0031, val_loss : 0.1178, acc : 0.958\n",
            "Epoch 91, loss : 0.0030, val_loss : 0.1182, acc : 0.958\n",
            "Epoch 92, loss : 0.0030, val_loss : 0.1186, acc : 0.958\n",
            "Epoch 93, loss : 0.0030, val_loss : 0.1180, acc : 0.958\n",
            "Epoch 94, loss : 0.0030, val_loss : 0.1194, acc : 0.958\n",
            "Epoch 95, loss : 0.0029, val_loss : 0.1197, acc : 0.958\n",
            "Epoch 96, loss : 0.0029, val_loss : 0.1196, acc : 0.958\n",
            "Epoch 97, loss : 0.0029, val_loss : 0.1201, acc : 0.944\n",
            "Epoch 98, loss : 0.0029, val_loss : 0.1210, acc : 0.944\n",
            "Epoch 99, loss : 0.0029, val_loss : 0.1207, acc : 0.944\n",
            "test_acc : 0.989\n"
          ]
        }
      ],
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "\n",
        "#Network change\n",
        "n_classes = 3\n",
        "\n",
        "#determine the shape of the network inputs and outputs\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    print(type(x))\n",
        "    print(type( weights['w1']))\n",
        "    layer_1 = tf.add(tf.matmul(x,  weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 推定結果\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7NhaTALadrx"
      },
      "source": [
        "[Problem 4] Create a House Prices model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Cn9nWZVkmqx",
        "outputId": "f2ab6959-29a7-4b85-e300-114aa05ba075"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-62-4f174bd32d45>:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = y.astype(np.int)[:, np.newaxis]\n"
          ]
        }
      ],
      "source": [
        "dataset_path =\"train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "y = np.log(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI7i6J-0ltrf",
        "outputId": "5318c83c-2b0c-4988-bd7c-3c5d325ed3fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-63-39a16a161d0a>:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : -58.5337, val_loss : -1432.1021, acc : 1.000\n",
            "Epoch 1, loss : -246.3585, val_loss : -3744.9675, acc : 1.000\n",
            "Epoch 2, loss : -583.7119, val_loss : -8278.9443, acc : 1.000\n",
            "Epoch 3, loss : -1188.3226, val_loss : -16033.1104, acc : 1.000\n",
            "Epoch 4, loss : -2184.1402, val_loss : -28297.4609, acc : 1.000\n",
            "Epoch 5, loss : -3678.3097, val_loss : -45977.8164, acc : 1.000\n",
            "Epoch 6, loss : -5765.9825, val_loss : -70181.3984, acc : 1.000\n",
            "Epoch 7, loss : -8563.4489, val_loss : -102046.2422, acc : 1.000\n",
            "Epoch 8, loss : -12204.5897, val_loss : -143021.1719, acc : 1.000\n",
            "Epoch 9, loss : -16779.8530, val_loss : -193537.8750, acc : 1.000\n",
            "Epoch 10, loss : -22354.7739, val_loss : -254561.4688, acc : 1.000\n",
            "Epoch 11, loss : -29014.4642, val_loss : -326654.6562, acc : 1.000\n",
            "Epoch 12, loss : -36793.3950, val_loss : -410080.9375, acc : 1.000\n",
            "Epoch 13, loss : -45726.2417, val_loss : -505247.6250, acc : 1.000\n",
            "Epoch 14, loss : -55854.4354, val_loss : -612577.8750, acc : 1.000\n",
            "Epoch 15, loss : -67240.1144, val_loss : -733168.1250, acc : 1.000\n",
            "Epoch 16, loss : -80010.8468, val_loss : -867623.8125, acc : 1.000\n",
            "Epoch 17, loss : -94141.0141, val_loss : -1015651.5625, acc : 1.000\n",
            "Epoch 18, loss : -109642.3905, val_loss : -1177530.7500, acc : 1.000\n",
            "Epoch 19, loss : -126542.8145, val_loss : -1353535.1250, acc : 1.000\n",
            "Epoch 20, loss : -144868.9354, val_loss : -1543923.8750, acc : 1.000\n",
            "Epoch 21, loss : -164645.9367, val_loss : -1748941.6250, acc : 1.000\n",
            "Epoch 22, loss : -185897.6187, val_loss : -1968820.1250, acc : 1.000\n",
            "Epoch 23, loss : -208646.5925, val_loss : -2203780.0000, acc : 1.000\n",
            "Epoch 24, loss : -232914.3596, val_loss : -2454031.5000, acc : 1.000\n",
            "Epoch 25, loss : -258721.5179, val_loss : -2719776.2500, acc : 1.000\n",
            "Epoch 26, loss : -286087.8373, val_loss : -3001208.5000, acc : 1.000\n",
            "Epoch 27, loss : -315032.3589, val_loss : -3298515.5000, acc : 1.000\n",
            "Epoch 28, loss : -345573.6004, val_loss : -3611880.2500, acc : 1.000\n",
            "Epoch 29, loss : -377729.5559, val_loss : -3941480.0000, acc : 1.000\n",
            "Epoch 30, loss : -411517.8156, val_loss : -4287489.0000, acc : 1.000\n",
            "Epoch 31, loss : -446955.5741, val_loss : -4650077.5000, acc : 1.000\n",
            "Epoch 32, loss : -484059.8011, val_loss : -5029415.0000, acc : 1.000\n",
            "Epoch 33, loss : -522847.1858, val_loss : -5425666.0000, acc : 1.000\n",
            "Epoch 34, loss : -563334.2677, val_loss : -5838995.5000, acc : 1.000\n",
            "Epoch 35, loss : -605537.4507, val_loss : -6269568.0000, acc : 1.000\n",
            "Epoch 36, loss : -649473.0230, val_loss : -6717545.0000, acc : 1.000\n",
            "Epoch 37, loss : -695157.1354, val_loss : -7183088.0000, acc : 1.000\n",
            "Epoch 38, loss : -742605.9465, val_loss : -7666358.5000, acc : 1.000\n",
            "Epoch 39, loss : -791835.6033, val_loss : -8167518.0000, acc : 1.000\n",
            "Epoch 40, loss : -842862.2104, val_loss : -8686727.0000, acc : 1.000\n",
            "Epoch 41, loss : -895701.8919, val_loss : -9224147.0000, acc : 1.000\n",
            "Epoch 42, loss : -950370.8105, val_loss : -9779941.0000, acc : 1.000\n",
            "Epoch 43, loss : -1006885.0953, val_loss : -10354270.0000, acc : 1.000\n",
            "Epoch 44, loss : -1065261.0300, val_loss : -10947295.0000, acc : 1.000\n",
            "Epoch 45, loss : -1125514.8148, val_loss : -11559180.0000, acc : 1.000\n",
            "Epoch 46, loss : -1187667.9197, val_loss : -12190468.0000, acc : 1.000\n",
            "Epoch 47, loss : -1251898.6178, val_loss : -12843612.0000, acc : 1.000\n",
            "Epoch 48, loss : -1318251.0792, val_loss : -13517170.0000, acc : 1.000\n",
            "Epoch 49, loss : -1386621.5728, val_loss : -14210819.0000, acc : 1.000\n",
            "Epoch 50, loss : -1457002.9486, val_loss : -14924622.0000, acc : 1.000\n",
            "Epoch 51, loss : -1529405.7687, val_loss : -15658702.0000, acc : 1.000\n",
            "Epoch 52, loss : -1603843.9400, val_loss : -16413215.0000, acc : 1.000\n",
            "Epoch 53, loss : -1680333.2270, val_loss : -17188316.0000, acc : 1.000\n",
            "Epoch 54, loss : -1758889.7334, val_loss : -17984178.0000, acc : 1.000\n",
            "Epoch 55, loss : -1839530.5343, val_loss : -18800966.0000, acc : 1.000\n",
            "Epoch 56, loss : -1922272.3126, val_loss : -19638850.0000, acc : 1.000\n",
            "Epoch 57, loss : -2007132.4325, val_loss : -20497996.0000, acc : 1.000\n",
            "Epoch 58, loss : -2094128.0128, val_loss : -21378590.0000, acc : 1.000\n",
            "Epoch 59, loss : -2183276.5696, val_loss : -22280800.0000, acc : 1.000\n",
            "Epoch 60, loss : -2274595.4818, val_loss : -23204802.0000, acc : 1.000\n",
            "Epoch 61, loss : -2368102.3212, val_loss : -24150770.0000, acc : 1.000\n",
            "Epoch 62, loss : -2463814.8587, val_loss : -25118886.0000, acc : 1.000\n",
            "Epoch 63, loss : -2561750.6424, val_loss : -26109322.0000, acc : 1.000\n",
            "Epoch 64, loss : -2661927.5653, val_loss : -27122256.0000, acc : 1.000\n",
            "Epoch 65, loss : -2764363.1906, val_loss : -28157866.0000, acc : 1.000\n",
            "Epoch 66, loss : -2869075.4625, val_loss : -29216340.0000, acc : 1.000\n",
            "Epoch 67, loss : -2976082.6296, val_loss : -30297846.0000, acc : 1.000\n",
            "Epoch 68, loss : -3085402.6767, val_loss : -31402568.0000, acc : 1.000\n",
            "Epoch 69, loss : -3197053.0214, val_loss : -32530688.0000, acc : 1.000\n",
            "Epoch 70, loss : -3311051.9679, val_loss : -33682388.0000, acc : 1.000\n",
            "Epoch 71, loss : -3427417.9036, val_loss : -34857836.0000, acc : 1.000\n",
            "Epoch 72, loss : -3546168.4882, val_loss : -36057236.0000, acc : 1.000\n",
            "Epoch 73, loss : -3667322.0214, val_loss : -37280752.0000, acc : 1.000\n",
            "Epoch 74, loss : -3790896.6017, val_loss : -38528564.0000, acc : 1.000\n",
            "Epoch 75, loss : -3916910.5653, val_loss : -39800852.0000, acc : 1.000\n",
            "Epoch 76, loss : -4045381.5011, val_loss : -41097820.0000, acc : 1.000\n",
            "Epoch 77, loss : -4176328.1028, val_loss : -42419628.0000, acc : 1.000\n",
            "Epoch 78, loss : -4309768.5139, val_loss : -43766468.0000, acc : 1.000\n",
            "Epoch 79, loss : -4445720.9593, val_loss : -45138516.0000, acc : 1.000\n",
            "Epoch 80, loss : -4584203.5889, val_loss : -46535952.0000, acc : 1.000\n",
            "Epoch 81, loss : -4725234.3469, val_loss : -47958980.0000, acc : 1.000\n",
            "Epoch 82, loss : -4868831.8844, val_loss : -49407752.0000, acc : 1.000\n",
            "Epoch 83, loss : -5015014.3726, val_loss : -50882472.0000, acc : 1.000\n",
            "Epoch 84, loss : -5163800.4839, val_loss : -52383312.0000, acc : 1.000\n",
            "Epoch 85, loss : -5315207.8073, val_loss : -53910456.0000, acc : 1.000\n",
            "Epoch 86, loss : -5469254.6381, val_loss : -55464088.0000, acc : 1.000\n",
            "Epoch 87, loss : -5625958.8651, val_loss : -57044392.0000, acc : 1.000\n",
            "Epoch 88, loss : -5785339.3062, val_loss : -58651548.0000, acc : 1.000\n",
            "Epoch 89, loss : -5947414.8994, val_loss : -60285740.0000, acc : 1.000\n",
            "Epoch 90, loss : -6112202.9807, val_loss : -61947152.0000, acc : 1.000\n",
            "Epoch 91, loss : -6279721.8030, val_loss : -63635968.0000, acc : 1.000\n",
            "Epoch 92, loss : -6449989.8801, val_loss : -65352364.0000, acc : 1.000\n",
            "Epoch 93, loss : -6623025.1949, val_loss : -67096524.0000, acc : 1.000\n",
            "Epoch 94, loss : -6798846.0857, val_loss : -68868632.0000, acc : 1.000\n",
            "Epoch 95, loss : -6977471.2463, val_loss : -70668864.0000, acc : 1.000\n",
            "Epoch 96, loss : -7158918.6938, val_loss : -72497432.0000, acc : 1.000\n",
            "Epoch 97, loss : -7343206.5696, val_loss : -74354488.0000, acc : 1.000\n",
            "Epoch 98, loss : -7530353.0493, val_loss : -76240216.0000, acc : 1.000\n",
            "Epoch 99, loss : -7720375.6360, val_loss : -78154784.0000, acc : 1.000\n",
            "test_acc : 1.000\n"
          ]
        }
      ],
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# 計算グラフに渡す引数の形を決める\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 推定結果\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGJ4LGt3l5nN"
      },
      "source": [
        "[Problem 5] Create an MNIST model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbNlWlIRmHr0",
        "outputId": "38546613-6c24-4c0f-86e9-cfb48652b431"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-68-b07ba356c83f>:4: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_train = X_train.astype(np.float)\n",
            "<ipython-input-68-b07ba356c83f>:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_test = X_test.astype(np.float)\n",
            "<ipython-input-68-b07ba356c83f>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_train = y_train.astype(np.int)[:, np.newaxis]\n",
            "<ipython-input-68-b07ba356c83f>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_test = y_test.astype(np.int)[:, np.newaxis]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)\n",
        "print(y_train_one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xO-Z5IsmYDr",
        "outputId": "150f25fd-049c-4ccf-a3cf-af491deb3497"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-69-1bc490271cac>:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.Tensor'>\n",
            "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
            "Epoch 0, loss : 0.4890, val_loss : 1.0169, acc : 0.959\n",
            "Epoch 1, loss : 0.0615, val_loss : 0.4362, acc : 0.968\n",
            "Epoch 2, loss : 0.0284, val_loss : 0.2467, acc : 0.973\n",
            "Epoch 3, loss : 0.0171, val_loss : 0.1760, acc : 0.975\n",
            "Epoch 4, loss : 0.0117, val_loss : 0.1368, acc : 0.977\n",
            "Epoch 5, loss : 0.0089, val_loss : 0.1137, acc : 0.979\n",
            "Epoch 6, loss : 0.0071, val_loss : 0.0980, acc : 0.982\n",
            "Epoch 7, loss : 0.0060, val_loss : 0.0934, acc : 0.981\n",
            "Epoch 8, loss : 0.0052, val_loss : 0.0890, acc : 0.982\n",
            "Epoch 9, loss : 0.0046, val_loss : 0.0874, acc : 0.983\n",
            "Epoch 10, loss : 0.0041, val_loss : 0.0854, acc : 0.983\n",
            "Epoch 11, loss : 0.0037, val_loss : 0.0818, acc : 0.984\n",
            "Epoch 12, loss : 0.0034, val_loss : 0.0790, acc : 0.984\n",
            "Epoch 13, loss : 0.0032, val_loss : 0.0790, acc : 0.984\n",
            "Epoch 14, loss : 0.0029, val_loss : 0.0762, acc : 0.985\n",
            "Epoch 15, loss : 0.0027, val_loss : 0.0770, acc : 0.985\n",
            "Epoch 16, loss : 0.0026, val_loss : 0.0746, acc : 0.986\n",
            "Epoch 17, loss : 0.0024, val_loss : 0.0739, acc : 0.986\n",
            "Epoch 18, loss : 0.0023, val_loss : 0.0754, acc : 0.986\n",
            "Epoch 19, loss : 0.0021, val_loss : 0.0782, acc : 0.986\n",
            "Epoch 20, loss : 0.0020, val_loss : 0.0764, acc : 0.986\n",
            "Epoch 21, loss : 0.0019, val_loss : 0.0743, acc : 0.987\n",
            "Epoch 22, loss : 0.0019, val_loss : 0.0761, acc : 0.987\n",
            "Epoch 23, loss : 0.0018, val_loss : 0.0772, acc : 0.987\n",
            "Epoch 24, loss : 0.0017, val_loss : 0.0768, acc : 0.988\n",
            "Epoch 25, loss : 0.0016, val_loss : 0.0784, acc : 0.987\n",
            "Epoch 26, loss : 0.0015, val_loss : 0.0776, acc : 0.988\n",
            "Epoch 27, loss : 0.0015, val_loss : 0.0777, acc : 0.988\n",
            "Epoch 28, loss : 0.0014, val_loss : 0.0820, acc : 0.988\n",
            "Epoch 29, loss : 0.0013, val_loss : 0.0789, acc : 0.988\n",
            "Epoch 30, loss : 0.0013, val_loss : 0.0799, acc : 0.988\n",
            "Epoch 31, loss : 0.0013, val_loss : 0.0811, acc : 0.988\n",
            "Epoch 32, loss : 0.0012, val_loss : 0.0785, acc : 0.989\n",
            "Epoch 33, loss : 0.0012, val_loss : 0.0799, acc : 0.989\n",
            "Epoch 34, loss : 0.0011, val_loss : 0.0828, acc : 0.989\n",
            "Epoch 35, loss : 0.0011, val_loss : 0.0856, acc : 0.988\n",
            "Epoch 36, loss : 0.0011, val_loss : 0.0852, acc : 0.989\n",
            "Epoch 37, loss : 0.0010, val_loss : 0.0846, acc : 0.989\n",
            "Epoch 38, loss : 0.0010, val_loss : 0.0875, acc : 0.988\n",
            "Epoch 39, loss : 0.0009, val_loss : 0.0869, acc : 0.989\n",
            "Epoch 40, loss : 0.0009, val_loss : 0.0880, acc : 0.989\n",
            "Epoch 41, loss : 0.0009, val_loss : 0.0906, acc : 0.989\n",
            "Epoch 42, loss : 0.0008, val_loss : 0.0917, acc : 0.989\n",
            "Epoch 43, loss : 0.0008, val_loss : 0.0925, acc : 0.989\n",
            "Epoch 44, loss : 0.0008, val_loss : 0.0911, acc : 0.989\n",
            "Epoch 45, loss : 0.0008, val_loss : 0.0933, acc : 0.989\n",
            "Epoch 46, loss : 0.0007, val_loss : 0.0945, acc : 0.989\n",
            "Epoch 47, loss : 0.0007, val_loss : 0.0947, acc : 0.989\n",
            "Epoch 48, loss : 0.0007, val_loss : 0.0986, acc : 0.989\n",
            "Epoch 49, loss : 0.0007, val_loss : 0.1020, acc : 0.989\n",
            "Epoch 50, loss : 0.0007, val_loss : 0.0974, acc : 0.989\n",
            "Epoch 51, loss : 0.0006, val_loss : 0.1005, acc : 0.989\n",
            "Epoch 52, loss : 0.0006, val_loss : 0.1021, acc : 0.989\n",
            "Epoch 53, loss : 0.0006, val_loss : 0.1012, acc : 0.989\n",
            "Epoch 54, loss : 0.0006, val_loss : 0.1031, acc : 0.990\n",
            "Epoch 55, loss : 0.0006, val_loss : 0.1053, acc : 0.990\n",
            "Epoch 56, loss : 0.0006, val_loss : 0.1047, acc : 0.990\n",
            "Epoch 57, loss : 0.0006, val_loss : 0.1095, acc : 0.990\n",
            "Epoch 58, loss : 0.0005, val_loss : 0.1107, acc : 0.989\n",
            "Epoch 59, loss : 0.0005, val_loss : 0.1137, acc : 0.989\n",
            "Epoch 60, loss : 0.0005, val_loss : 0.1129, acc : 0.989\n",
            "Epoch 61, loss : 0.0005, val_loss : 0.1132, acc : 0.989\n",
            "Epoch 62, loss : 0.0005, val_loss : 0.1151, acc : 0.990\n",
            "Epoch 63, loss : 0.0005, val_loss : 0.1151, acc : 0.990\n",
            "Epoch 64, loss : 0.0005, val_loss : 0.1112, acc : 0.990\n",
            "Epoch 65, loss : 0.0005, val_loss : 0.1159, acc : 0.990\n",
            "Epoch 66, loss : 0.0005, val_loss : 0.1140, acc : 0.990\n",
            "Epoch 67, loss : 0.0005, val_loss : 0.1170, acc : 0.990\n",
            "Epoch 68, loss : 0.0004, val_loss : 0.1137, acc : 0.990\n",
            "Epoch 69, loss : 0.0004, val_loss : 0.1188, acc : 0.990\n",
            "Epoch 70, loss : 0.0004, val_loss : 0.1198, acc : 0.990\n",
            "Epoch 71, loss : 0.0004, val_loss : 0.1213, acc : 0.990\n",
            "Epoch 72, loss : 0.0004, val_loss : 0.1241, acc : 0.990\n",
            "Epoch 73, loss : 0.0004, val_loss : 0.1210, acc : 0.990\n",
            "Epoch 74, loss : 0.0004, val_loss : 0.1183, acc : 0.990\n",
            "Epoch 75, loss : 0.0004, val_loss : 0.1197, acc : 0.990\n",
            "Epoch 76, loss : 0.0004, val_loss : 0.1253, acc : 0.990\n",
            "Epoch 77, loss : 0.0004, val_loss : 0.1232, acc : 0.990\n",
            "Epoch 78, loss : 0.0004, val_loss : 0.1264, acc : 0.990\n",
            "Epoch 79, loss : 0.0004, val_loss : 0.1238, acc : 0.990\n",
            "Epoch 80, loss : 0.0004, val_loss : 0.1262, acc : 0.990\n",
            "Epoch 81, loss : 0.0004, val_loss : 0.1227, acc : 0.990\n",
            "Epoch 82, loss : 0.0003, val_loss : 0.1296, acc : 0.990\n",
            "Epoch 83, loss : 0.0003, val_loss : 0.1297, acc : 0.990\n",
            "Epoch 84, loss : 0.0003, val_loss : 0.1352, acc : 0.990\n",
            "Epoch 85, loss : 0.0003, val_loss : 0.1396, acc : 0.989\n",
            "Epoch 86, loss : 0.0003, val_loss : 0.1289, acc : 0.990\n",
            "Epoch 87, loss : 0.0003, val_loss : 0.1334, acc : 0.990\n",
            "Epoch 88, loss : 0.0003, val_loss : 0.1341, acc : 0.990\n",
            "Epoch 89, loss : 0.0003, val_loss : 0.1279, acc : 0.990\n",
            "Epoch 90, loss : 0.0003, val_loss : 0.1340, acc : 0.990\n",
            "Epoch 91, loss : 0.0003, val_loss : 0.1308, acc : 0.990\n",
            "Epoch 92, loss : 0.0003, val_loss : 0.1385, acc : 0.990\n",
            "Epoch 93, loss : 0.0003, val_loss : 0.1369, acc : 0.990\n",
            "Epoch 94, loss : 0.0003, val_loss : 0.1448, acc : 0.990\n",
            "Epoch 95, loss : 0.0003, val_loss : 0.1402, acc : 0.990\n",
            "Epoch 96, loss : 0.0003, val_loss : 0.1423, acc : 0.990\n",
            "Epoch 97, loss : 0.0003, val_loss : 0.1412, acc : 0.990\n",
            "Epoch 98, loss : 0.0003, val_loss : 0.1454, acc : 0.990\n",
            "Epoch 99, loss : 0.0003, val_loss : 0.1503, acc : 0.990\n",
            "test_acc : 0.991\n"
          ]
        }
      ],
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "\n",
        "#Network change\n",
        "n_classes = 10\n",
        "\n",
        "#determine the shape of the network inputs and outputs\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    print(type(x))\n",
        "    print(type( weights['w1']))\n",
        "    layer_1 = tf.add(tf.matmul(x,  weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 推定結果\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOOeZvbq02QtovTse6uaCOV",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
