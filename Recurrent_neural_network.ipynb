{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pCv6-OlYsQ4X"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvob3RHewIrI"
      },
      "source": [
        "[Problem 1] Forward propagation implementation of SimpleRNN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MOZrV1ozneJ"
      },
      "source": [
        "Create a SimpleRNN class SimpleRNN. The basic structure will be the same as the FC class.\n",
        "\n",
        "The forward propagation formula looks like this: It also describes what the shape of ndarray will be.\n",
        "\n",
        "We denote the batch size batch_size, the number of input features n_features, and the number of RNN nodes . n_nodesThe activation function proceeds as tanh, but it can be replaced with ReLU, etc., as in previous neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMqIgV2dtwJG"
      },
      "source": [
        "##Activation functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QBVOaRqitrIG"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return self.sigmoid(A)\n",
        "    def backward(self, dZ):\n",
        "        _sig = self.sigmoid(self.A)\n",
        "        return dZ * (1 - _sig)*_sig\n",
        "    def sigmoid(self, X):\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(A)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - (np.tanh(self.A))**2)\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, X):\n",
        "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "        return self.Z\n",
        "    def backward(self, Y):\n",
        "        self.loss = self.loss_func(Y)\n",
        "        return self.Z - Y\n",
        "    def loss_func(self, Y, Z=None):\n",
        "        if Z is None:\n",
        "            Z = self.Z\n",
        "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.clip(A, 0, None)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * np.clip(np.sign(self.A), 0, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hIcyjQhGtqRP"
      },
      "outputs": [],
      "source": [
        "class SimpleRNN:\n",
        "    def __init__(self, x, w_h, w_x,b):\n",
        "        self.w_h = w_h\n",
        "        self.w_x = w_x\n",
        "        self.b = b\n",
        "        self.batch_size = x.shape[0] # 1\n",
        "        self.n_sequences = x.shape[1] # 3\n",
        "        self.n_features = x.shape[2] # 2\n",
        "        self.n_nodes = w_x.shape[1] # 4\n",
        "        self.h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
        "        self.b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
        "\n",
        "    def forward(x,h):\n",
        "        for n in range(n_sequences):\n",
        "            h = np.tanh(x[:, n, :] @ w_x + h @ w_h + b)\n",
        "        return h\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dZ = dA@self.W.T\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        self.dW = self.X.T@dA\n",
        "        self.optimizer.update(self)\n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DEFKctCzqjK"
      },
      "source": [
        "[Problem 2] Forward propagation experiment with small sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OupRYt7KzttL"
      },
      "source": [
        "Consider forward propagation on small arrays.\n",
        "\n",
        "Let input x, initial state h, weights w_x and w_h, bias b be:\n",
        "\n",
        "Here the axes of the array x are in order of batch size, number of series, and number of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gywsvrRwDzF",
        "outputId": "b65b6a75-f64f-4773-f2f3-690a340b612d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
        "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
        "\n",
        "RNN = SimpleRNN(x=x, w_h=w_h, w_x=w_x, b=0.1)\n",
        "\n",
        "answer = SimpleRNN.forward(x,h)\n",
        "answer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMLwUXDwLz0qXni3EeqIQQb",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
